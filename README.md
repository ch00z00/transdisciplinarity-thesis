# Tarns-discriplinarity thesis

本論文は、立命館大学の教養科目であり先端科目である「超領域リベラルアーツ」における最終成果物である。
テーマは AI のハルシネーションであり、ハルシネーションと記号創発システム科学を結びつけて新たな記号創発に繋げるための仮説を提唱している。

<details>
<summary>English version</summary>

This thesis is the final product of Transdisciplinary Liberal Arts, a liberal arts and cutting-edge subject at Ritsumeikan University.
The theme is the hallucination of AI, and I'm proposing a hypothesis to link hallucination and symbol emergence system science to lead to the emergence of new symbols.

</details>

<br/>

## Abstract

ChatGPT などのディープラーニング・エージェントを使っていると誰しも体験することだが、AI はたびたびもっともらしい嘘をつく。それらは「ハルシネーション(幻覚)」と呼ばれ、ChatGPT の開発においては現時点(Jul 5, 2023)で最新モデルである GPT-4 でさえも改善の余地がまだまだあると OpenAI も公式に言及している。それを馬鹿にして楽しむ風潮は面白いとも思うが、嘲笑している私たち人間も同じようにもっともらしい嘘をついて暮らしている。
人間の方が優れている部分はまだ多分にあるが、嘘をつくことに関して言えば人間と相違ないと言える。
この論文では第一章から第三章にかけて、ディープラーニングのもととなっている人間の脳における「自由エネルギー原理」から、人間の認知や予測について説明しつつ、最終章である第四章では、記号創発システム科学における AI のハルシネーションの実用可能性について言及している。
ネガティブなものとして捉えられがちなハルシネーションをあえてポジティブに捉え直そうというスタンスに立ち、誰も(人間も ChatGPT でさえも)思いつかないような新たな記号を創発する種は、事実や一般的に正しいとされることのみならず、ハルシネーションにも見出せるのではないかと考えてこのような提案をした。
今後の展望としては、嘘しか吐かないエージェントを作り、嘘で溢れた中で新たな記号を創り出すような小規模なコミュニティを作りたいと思っている。

<details>
<summary>English version</summary>

As anyone using deep learning agents such as ChatGPT has experienced, AI often tells plausible lies. These are called "hallucinations," and OpenAI has also officially stated that there is still room for improvement in the development of ChatGPT, even with GPT-4, which is the latest model at the moment (Jul 5, 2023). are doing. I think it's funny that people are making fun of it, but we humans who are making fun of it are also living a plausible lie.
Although there are still many areas in which humans are superior, it can be said that they are no different from humans when it comes to lying.
In this thesis, from Chapters 1 to 3, I will explain human cognition and prediction based on the "free energy principle" in the human brain, which is the basis of deep learning, and in the fourth and final chapter, I will explain human cognition and prediction. , mentions the practical possibility of AI hallucination in symbol emergent systems science.
Taking the stance of reconsidering hallucination, which is often seen as a negative thing, in a positive light, the species that emerges with new symbols that no one (not even humans or ChatGPT) would have thought of is based on facts and general knowledge. I made this suggestion because I thought that it could be found not only in what is considered to be correct, but also in hallucination.
Looking to the future, I would like to create agents that only tell lies, and create a small community that can create new symbols in a world full of lies.

</details>

<br/>

## Index

はじめに

第一章　 InstructGPT 及び ChatGPT の仕組み

第二章　人間の予測プロセス

第三章　人間の推論と嘘

第四章　ハルシネーションの実用性

おわりに

<details>
<summary>English version</summary>

Introduction

Chapter 1 　 InstructGPT and ChatGPT mechanism

Chapter 2 　 Human predictive process

Chapter 3 　 Human reasoning and lying

Chapter 4 　 The practicality of hallucination

Conclusions

</details>

<br/>

## Links

[超領域リベラルアーツ](https://www.ritsumei.ac.jp/liberalarts/transdisciplinarity/)

[記号創発システム科学創成：実世界人工知能と
次世代共生社会の学術融合研究拠点](https://www.ritsumei.ac.jp/rgiro/project/fourth/taniguchi/)
